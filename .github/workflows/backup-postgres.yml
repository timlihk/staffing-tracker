name: Backup Railway Postgres - Hourly

on:
  schedule:
    - cron: '0 * * * *'   # Every hour at minute 0
  workflow_dispatch:      # Allows manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Install PostgreSQL 17 client
        run: |
          sudo apt-get update
          sudo apt-get install -y wget gnupg
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17

      - name: Dump database
        id: dump
        env:
          DATABASE_URL: ${{ secrets.RAILWAY_DATABASE_URL }}
        run: |
          ts=$(date -u +'%Y-%m-%dT%H-%M-%SZ')
          outfile="railway-backup-hourly-${ts}.dump"
          /usr/lib/postgresql/17/bin/pg_dump "$DATABASE_URL" \
            --format=custom \
            --no-owner \
            --no-privileges \
            --file="$outfile"
          gzip "$outfile"
          echo "backup_path=${outfile}.gz" >> $GITHUB_OUTPUT
          echo "backup_file=${outfile}" >> $GITHUB_OUTPUT

      - name: Validate backup integrity
        run: |
          backup_file="${{ steps.dump.outputs.backup_path }}"

          # Check file exists and is not empty
          if [ ! -f "$backup_file" ]; then
            echo "ERROR: Backup file not found"
            exit 1
          fi

          # Check minimum size (should be at least 1KB)
          size=$(stat -f%z "$backup_file" 2>/dev/null || stat -c%s "$backup_file")
          if [ "$size" -lt 1024 ]; then
            echo "ERROR: Backup file too small ($size bytes), likely corrupt"
            exit 1
          fi

          # Verify gzip integrity
          if ! gzip -t "$backup_file"; then
            echo "ERROR: Gzip integrity check failed"
            exit 1
          fi

          # Extract and validate pg_dump format
          gunzip -c "$backup_file" > /tmp/test.dump
          if ! /usr/lib/postgresql/17/bin/pg_restore --list /tmp/test.dump > /dev/null 2>&1; then
            echo "ERROR: pg_restore validation failed - backup may be corrupt"
            rm /tmp/test.dump
            exit 1
          fi
          rm /tmp/test.dump

          echo "‚úÖ Backup validation passed (size: $size bytes)"

      - name: Install rclone
        run: |
          curl https://rclone.org/install.sh | sudo bash

      - name: Configure rclone
        env:
          RCLONE_CONFIG_BASE64: ${{ secrets.RCLONE_CONFIG }}
        run: |
          mkdir -p ~/.config/rclone
          echo "$RCLONE_CONFIG_BASE64" | base64 -d > ~/.config/rclone/rclone.conf

          # Verify rclone can read it
          echo "‚úÖ Rclone configuration loaded"
          rclone config show

      - name: Upload to Google Drive
        run: |
          backup_file="${{ steps.dump.outputs.backup_path }}"

          # Create folder structure: staffing-tracker-backups/hourly/YYYY/MM/
          year=$(date -u +'%Y')
          month=$(date -u +'%m')
          remote_path="staffing-tracker-backups/hourly/$year/$month/"

          echo "üì§ Uploading to Google Drive: $remote_path"
          rclone copy "$backup_file" "gdrive:$remote_path" -v

          if [ $? -eq 0 ]; then
            echo "‚úÖ Successfully uploaded to Google Drive"
          else
            echo "‚ö†Ô∏è Failed to upload to Google Drive, but backup is still in GitHub artifacts"
            exit 0  # Don't fail the workflow
          fi

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: railway-postgres-hourly-${{ github.run_id }}
          path: ${{ steps.dump.outputs.backup_path }}
          retention-days: 1   # Keep hourly backups for 1 day
